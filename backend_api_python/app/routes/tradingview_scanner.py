"""
TradingView Scanner APIè·¯ç”±
æä¾›æ— éœ€ç™»å½•çš„å¤§è§„æ¨¡åŠ å¯†è´§å¸æ•°æ®è·å–æ¥å£
"""
from flask import Blueprint, request, jsonify
from datetime import datetime, timedelta
from app.services.tradingview_scanner_service import (
    TradingViewScannerAPI,
    get_top_perpetuals,
    get_default_watchlist,
    get_top_gainers
)
from app.services.tradingview_cache import get_cache_manager as get_tv_cache_manager_impl
from app import get_tv_cache_manager
from app.utils.logger import get_logger
from app import get_redis_client
from app.services.hama_calculator import calculate_hama_from_ohlcv
from app.services.screenshot_cache import get_screenshot_cache
import json
import os
import threading
import time

logger = get_logger(__name__)

tradingview_scanner_bp = Blueprint('tradingview_scanner', __name__)

# å…³æ³¨åˆ—è¡¨ç¼“å­˜
_watchlist_cache = {
    'data': None,
    'timestamp': None,
    'duration': timedelta(minutes=5)  # ç¼“å­˜5åˆ†é’Ÿ
}

# HAMA ç¼“å­˜ TTL (5åˆ†é’Ÿ)
_HAMA_CACHE_TTL = 300

# æˆªå›¾ç¼“å­˜ TTL (10åˆ†é’Ÿ)
_SCREENSHOT_CACHE_TTL = 600

# æˆªå›¾ç¼“å­˜ Worker çº¿ç¨‹
_screenshot_worker_thread = None
_screenshot_worker_running = False


def _get_hama_from_cache(symbol: str) -> dict:
    """ä» Redis ç¼“å­˜è·å– HAMA çŠ¶æ€"""
    try:
        redis_client = get_redis_client()
        if not redis_client:
            return None

        cache_key = f"hama_status:{symbol}"
        cached_data = redis_client.get(cache_key)

        if cached_data:
            logger.info(f"âœ… {symbol} ä» Redis ç¼“å­˜è·å– HAMA çŠ¶æ€")
            return json.loads(cached_data)

        return None
    except Exception as e:
        logger.error(f"ä» Redis è·å– HAMA ç¼“å­˜å¤±è´¥: {e}")
        return None


def _set_hama_to_cache(symbol: str, hama_data: dict):
    """å°† HAMA çŠ¶æ€å­˜å…¥ Redis ç¼“å­˜"""
    try:
        redis_client = get_redis_client()
        if not redis_client:
            return

        cache_key = f"hama_status:{symbol}"
        redis_client.setex(
            cache_key,
            _HAMA_CACHE_TTL,
            json.dumps(hama_data, ensure_ascii=False)
        )
        logger.info(f"âœ… {symbol} HAMA çŠ¶æ€å·²ç¼“å­˜")
    except Exception as e:
        logger.error(f"è®¾ç½® HAMA ç¼“å­˜å¤±è´¥: {e}")


def _get_screenshot_from_cache(symbol: str, interval: str = '15m') -> str:
    """ä»æ•°æ®åº“ç¼“å­˜è·å–æˆªå›¾ base64 æ•°æ® (ä¼˜å…ˆæ•°æ®åº“,å¤‡ç”¨Redis)"""
    try:
        # ä¼˜å…ˆä»æ•°æ®åº“è·å–
        screenshot_cache = get_screenshot_cache()
        cached_data = screenshot_cache.get_screenshot(symbol, interval)

        if cached_data and cached_data.get('image_base64'):
            logger.info(f"âœ… {symbol} ä»æ•°æ®åº“ç¼“å­˜è·å–æˆªå›¾")
            return cached_data['image_base64']

        # å¦‚æœæ•°æ®åº“æ²¡æœ‰,å°è¯•ä»Redisè·å– (å…¼å®¹æ—§ç‰ˆæœ¬)
        redis_client = get_redis_client()
        if redis_client:
            cache_key = f"chart_screenshot:{symbol}:{interval}"
            cached_data = redis_client.get(cache_key)

            if cached_data:
                logger.info(f"âœ… {symbol} ä» Redis ç¼“å­˜è·å–æˆªå›¾ (å¤‡ç”¨)")
                # å°†Redisæ•°æ®è¿ç§»åˆ°æ•°æ®åº“
                screenshot_cache.save_screenshot(symbol, interval, cached_data.decode('utf-8'))
                return cached_data.decode('utf-8')

        return None
    except Exception as e:
        logger.error(f"ä»ç¼“å­˜è·å–æˆªå›¾å¤±è´¥: {e}")
        return None


def _save_screenshot_to_cache(symbol: str, interval: str, image_base64: str,
                              file_size: int = None, screenshot_url: str = None) -> bool:
    """ä¿å­˜æˆªå›¾ base64 æ•°æ®åˆ°æ•°æ®åº“ç¼“å­˜ (åŒæ—¶ä¿å­˜åˆ°Redisä½œä¸ºå¿«é€Ÿç¼“å­˜)"""
    try:
        # ä¿å­˜åˆ°æ•°æ®åº“ (æ°¸ä¹…å­˜å‚¨,ç›´åˆ°æ‰‹åŠ¨æ¸…ç†)
        screenshot_cache = get_screenshot_cache()
        success = screenshot_cache.save_screenshot(symbol, interval, image_base64, file_size, screenshot_url)

        if success:
            # åŒæ—¶ä¿å­˜åˆ°Redisä½œä¸ºå¿«é€Ÿç¼“å­˜ (TTL 10åˆ†é’Ÿ)
            redis_client = get_redis_client()
            if redis_client:
                cache_key = f"chart_screenshot:{symbol}:{interval}"
                redis_client.setex(cache_key, _SCREENSHOT_CACHE_TTL, image_base64)
                logger.info(f"âœ… {symbol} æˆªå›¾å·²ç¼“å­˜åˆ°æ•°æ®åº“ + Redis (TTL: {_SCREENSHOT_CACHE_TTL}ç§’)")
            else:
                logger.info(f"âœ… {symbol} æˆªå›¾å·²ç¼“å­˜åˆ°æ•°æ®åº“")

        return success
    except Exception as e:
        logger.error(f"ä¿å­˜æˆªå›¾åˆ°ç¼“å­˜å¤±è´¥: {e}")
        return False


def _parse_cookie_string(cookie_string: str) -> list:
    """
    è§£æ cookie å­—ç¬¦ä¸²ä¸º Playwright æ‰€éœ€çš„æ ¼å¼
    
    Args:
        cookie_string: ä» CLAUDE.md ä¸­è¯»å–çš„ cookie å­—ç¬¦ä¸²
    
    Returns:
        æ ¼å¼åŒ–åçš„ cookie åˆ—è¡¨
    """
    cookies = []
    for cookie_pair in cookie_string.split(';'):
        cookie_pair = cookie_pair.strip()
        if not cookie_pair:
            continue
        if '=' in cookie_pair:
            name, value = cookie_pair.split('=', 1)
            cookies.append({
                'name': name,
                'value': value,
                'domain': '.tradingview.com',
                'path': '/',
                'expires': -1,
                'httpOnly': True,
                'secure': True
            })
    return cookies


def _get_tradingview_cookies() -> list:
    """
    ä» CLAUDE.md æ–‡ä»¶ä¸­è·å– TradingView cookie
    
    Returns:
        æ ¼å¼åŒ–åçš„ cookie åˆ—è¡¨
    """
    try:
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        claude_md_path = os.path.join(project_root, 'CLAUDE.md')
        
        with open(claude_md_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æŸ¥æ‰¾ cookie éƒ¨åˆ†
        cookie_start = content.find('# cookie')
        if cookie_start == -1:
            logger.warning("CLAUDE.md ä¸­æœªæ‰¾åˆ° cookie éƒ¨åˆ†")
            return []
        
        # æå– cookie å­—ç¬¦ä¸²
        cookie_section = content[cookie_start:]
        cookie_lines = cookie_section.split('\n')
        cookie_string = ''
        for line in cookie_lines[1:]:
            line = line.strip()
            if line and not line.startswith('#') and not line.startswith('```'):
                cookie_string = line
                break
        
        if not cookie_string:
            logger.warning("CLAUDE.md ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„ cookie å­—ç¬¦ä¸²")
            return []
        
        # è§£æ cookie
        cookies = _parse_cookie_string(cookie_string)
        logger.info(f"âœ… ä» CLAUDE.md ä¸­åŠ è½½äº† {len(cookies)} ä¸ª cookie")
        return cookies
    except Exception as e:
        logger.error(f"è¯»å–æˆ–è§£æ cookie å¤±è´¥: {e}")
        return []


def _capture_and_cache_screenshot(symbol: str, interval: str = '15') -> tuple[bool, str | None]:
    """
    æˆªå›¾å¹¶ç¼“å­˜åˆ° Redis (ä¸ä½¿ç”¨ OCR,ä»…æˆªå›¾)
    """
    try:
        from playwright.sync_api import sync_playwright
        from playwright_stealth.stealth import Stealth
        import base64
        import os

        logger.info(f"æ­£åœ¨æˆªå– {symbol} å›¾è¡¨...")

        # è½¬æ¢ interval æ ¼å¼: 15m -> 15, 1h -> 60, 1d -> 1D
        interval_mapping = {
            '1m': '1', '3m': '3', '5m': '5', '15m': '15', '30m': '30',
            '1h': '60', '2h': '120', '4h': '240', '6h': '360', '12h': '720',
            '1d': 'D', '1w': 'W', '1M': 'M'
        }
        tv_interval = interval_mapping.get(interval, interval)

        # æ„å»º TradingView å›¾è¡¨ URL - ä½¿ç”¨ widget embed URL (ä¸éœ€è¦ç™»å½•)
        # æ ¼å¼: https://s.tradingview.com/widgetembed/?frameElementId=tradingview_76d87&symbol=BINANCE%3ABTCUSDT&interval=15
        chart_url = f"https://s.tradingview.com/widgetembed/?frameElementId=tradingview_widget&symbol=BINANCE:{symbol}&interval={tv_interval}&hidesidetoolbar=1&symboledit=1&saveimage=0&toolbarbg=f1f3f6&studies=%5B%5D&theme=light&style=1&timezone=Etc%2FUTC"

        # æˆªå›¾è·¯å¾„ - ä¿®æ”¹ä¸ºä¿å­˜åˆ°é¡¹ç›®æ ¹ç›®å½•çš„ screenshot ç›®å½•
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        screenshot_dir = os.path.join(project_root, 'screenshot')
        os.makedirs(screenshot_dir, exist_ok=True)
        screenshot_path = os.path.join(screenshot_dir, f"{symbol}_{interval}_chart.png")
        logger.info(f"æˆªå›¾å°†ä¿å­˜åˆ°: {screenshot_path}")

        logger.info(f"TradingView Widget URL: {chart_url}")

        # ä» CLAUDE.md è·å– cookie
        cookies = _get_tradingview_cookies()

        # ä½¿ç”¨ Playwright ç›´æ¥æˆªå›¾,ä¸åˆå§‹åŒ– OCR
        with sync_playwright() as p:
            # é…ç½®ä»£ç†
            proxy_url = os.environ.get('PROXY_URL')

            browser = p.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-gpu',
                    f'--proxy-server={proxy_url}' if proxy_url else ''
                ]
            )

            context = browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            )
            
            # è®¾ç½® cookie
            if cookies:
                context.add_cookies(cookies)
                logger.info(f"âœ… å·²è®¾ç½® {len(cookies)} ä¸ª TradingView cookie")
            
            page = context.new_page()

            # åº”ç”¨ stealth æ¨¡å¼
            stealth_config = Stealth()
            stealth_config.apply_stealth_sync(page)

            # è®¿é—®å›¾è¡¨ - ä½¿ç”¨æ›´å®½æ¾çš„è¶…æ—¶è®¾ç½®
            logger.info(f"æ­£åœ¨åŠ è½½ {symbol} å›¾è¡¨...")
            try:
                # å…ˆç­‰å¾… DOMContentLoaded,ä¸ç­‰å¾…æ‰€æœ‰èµ„æºåŠ è½½
                page.goto(chart_url, timeout=90000, wait_until='domcontentloaded')

                # ç­‰å¾…å›¾è¡¨å®¹å™¨å‡ºç°
                logger.info(f"ç­‰å¾… {symbol} å›¾è¡¨å®¹å™¨åŠ è½½...")
                try:
                    page.wait_for_selector('div[class*="chart-container"]', timeout=15000)
                except:
                    # å¦‚æœæ‰¾ä¸åˆ° chart-container,ç­‰å¾…ä»»æ„ div å‡ºç°
                    logger.warning(f"{symbol} æœªæ‰¾åˆ° chart-container,ç­‰å¾…é¡µé¢å…ƒç´ ...")
                    page.wait_for_selector('body', timeout=10000)

                # é¢å¤–ç­‰å¾…å›¾è¡¨æ¸²æŸ“å®Œæˆ - å¢åŠ ç­‰å¾…æ—¶é—´åˆ°15ç§’
                logger.info(f"ç­‰å¾… {symbol} å›¾è¡¨æ¸²æŸ“...")
                page.wait_for_timeout(15000)

            except Exception as e:
                logger.warning(f"{symbol} é¡µé¢åŠ è½½è­¦å‘Š: {e},ç»§ç»­å°è¯•æˆªå›¾...")
                # å³ä½¿ç­‰å¾…å¤±è´¥ä¹Ÿç»§ç»­,å°è¯•æˆªå›¾

            # æˆªå›¾ - æˆªå–é¡µé¢å³ä¾§å›¾è¡¨åŒºåŸŸ
            logger.info(f"æˆªå– {symbol} å›¾è¡¨åˆ°: {screenshot_path}")

            try:
                # å…ˆå°è¯•æˆªå–å®Œæ•´é¡µé¢,ä¾¿äºè°ƒè¯•
                logger.info(f"æˆªå– {symbol} å®Œæ•´é¡µé¢...")
                page.screenshot(path=screenshot_path, full_page=False)
                logger.info(f"âœ… {symbol} å®Œæ•´é¡µé¢æˆªå›¾å®Œæˆ")
            except Exception as e:
                logger.error(f"{symbol} æˆªå›¾å¤±è´¥: {e}")
                browser.close()
                return False

            browser.close()

        # æ£€æŸ¥æˆªå›¾æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æœ‰å†…å®¹
        if not os.path.exists(screenshot_path):
            logger.error(f"{symbol} æˆªå›¾æ–‡ä»¶ä¸å­˜åœ¨")
            return False

        file_size = os.path.getsize(screenshot_path)
        if file_size < 1000:  # å°äº1KBè®¤ä¸ºæˆªå›¾å¤±è´¥
            logger.error(f"{symbol} æˆªå›¾æ–‡ä»¶è¿‡å°: {file_size} bytes")
            return False

        # å°†æˆªå›¾è½¬æ¢ä¸º base64
        with open(screenshot_path, 'rb') as f:
            image_data = f.read()
            image_base64 = base64.b64encode(image_data).decode('utf-8')

        # ä¿å­˜åˆ°æ•°æ®åº“ + Redis ç¼“å­˜
        _save_screenshot_to_cache(symbol, interval, image_base64, file_size, chart_url)
        logger.info(f"âœ… {symbol} æˆªå›¾å¹¶ç¼“å­˜æˆåŠŸ (å¤§å°: {file_size} bytes)")
        return True

    except Exception as e:
        logger.error(f"æˆªå›¾å¹¶ç¼“å­˜å¤±è´¥: {e}", exc_info=True)
        return False


def _cache_all_gainers_screenshots():
    """ç¼“å­˜æ‰€æœ‰æ¶¨å¹…æ¦œå¸ç§çš„æˆªå›¾"""
    try:
        logger.info("ğŸš€ å¼€å§‹ç¼“å­˜æ¶¨å¹…æ¦œæ‰€æœ‰å¸ç§æˆªå›¾...")

        # è·å–æ¶¨å¹…æ¦œå‰10
        gainers = get_top_gainers(limit=10)
        if not gainers:
            logger.error("âŒ è·å–æ¶¨å¹…æ¦œæ•°æ®å¤±è´¥")
            return

        logger.info(f"ğŸ“Š æ¶¨å¹…æ¦œå¸ç§æ•°é‡: {len(gainers)}")

        # ä¸ºæ¯ä¸ªå¸ç§æˆªå›¾å¹¶ç¼“å­˜
        success_count = 0
        failed_count = 0

        for coin in gainers:
            symbol = coin.get('symbol')
            if not symbol:
                continue

            try:
                if _capture_and_cache_screenshot(symbol, '15m'):
                    success_count += 1
                else:
                    failed_count += 1

                # é¿å…è¯·æ±‚è¿‡å¿«,ç­‰å¾…1ç§’
                time.sleep(1)

            except Exception as e:
                logger.error(f"å¤„ç† {symbol} æˆªå›¾æ—¶å‡ºé”™: {e}")
                failed_count += 1

        logger.info(f"âœ… ç¼“å­˜å®Œæˆ - æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}")

    except Exception as e:
        logger.error(f"ç¼“å­˜æ‰€æœ‰æˆªå›¾å¤±è´¥: {e}", exc_info=True)


def _screenshot_worker():
    """æˆªå›¾ç¼“å­˜ Worker çº¿ç¨‹ - æ¯10åˆ†é’Ÿåˆ·æ–°ä¸€æ¬¡"""
    global _screenshot_worker_running

    logger.info("ğŸ”„ æˆªå›¾ç¼“å­˜ Worker å·²å¯åŠ¨")

    while _screenshot_worker_running:
        try:
            # æ‰§è¡Œç¼“å­˜
            _cache_all_gainers_screenshots()

            # ç­‰å¾…10åˆ†é’Ÿ
            logger.info("â° ä¸‹æ¬¡åˆ·æ–°å°†åœ¨10åˆ†é’Ÿå...")
            for _ in range(600):  # 10åˆ†é’Ÿ = 600ç§’
                if not _screenshot_worker_running:
                    break
                time.sleep(1)

        except Exception as e:
            logger.error(f"æˆªå›¾ç¼“å­˜ Worker å‡ºé”™: {e}", exc_info=True)
            # å‡ºé”™åç­‰å¾…1åˆ†é’Ÿå†è¯•
            for _ in range(60):
                if not _screenshot_worker_running:
                    break
                time.sleep(1)

    logger.info("ğŸ›‘ æˆªå›¾ç¼“å­˜ Worker å·²åœæ­¢")


def start_screenshot_worker():
    """å¯åŠ¨æˆªå›¾ç¼“å­˜ Worker"""
    global _screenshot_worker_thread, _screenshot_worker_running

    if _screenshot_worker_running:
        logger.warning("âš ï¸ æˆªå›¾ç¼“å­˜ Worker å·²ç»åœ¨è¿è¡Œ")
        return

    _screenshot_worker_running = True
    _screenshot_worker_thread = threading.Thread(target=_screenshot_worker, daemon=True)
    _screenshot_worker_thread.start()
    logger.info("âœ… æˆªå›¾ç¼“å­˜ Worker å·²å¯åŠ¨")


def stop_screenshot_worker():
    """åœæ­¢æˆªå›¾ç¼“å­˜ Worker"""
    global _screenshot_worker_running

    _screenshot_worker_running = False
    logger.info("ğŸ›‘ æˆªå›¾ç¼“å­˜ Worker åœæ­¢ä¿¡å·å·²å‘é€")


def _estimate_hama_status(coin_data: dict, use_cache: bool = True) -> dict:
    """
    ç©ºå‡½æ•° - å·²ç§»é™¤ HAMA çŠ¶æ€è®¡ç®—

    Args:
        coin_data: å¸ç§æ•°æ®
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ (é»˜è®¤ True)

    Returns:
        ç©ºå­—å…¸
    """
    # ä¸å†è®¡ç®— HAMA çŠ¶æ€,ç›´æ¥è¿”å›ç©ºå­—å…¸
    return {}


def _estimate_hama_status_simple(coin_data: dict) -> dict:
    """
    åŸºäºä»·æ ¼æ•°æ®ç®€å•ä¼°ç®— HAMA çŠ¶æ€ (å¤‡ç”¨æ–¹æ¡ˆ)

    Args:
        coin_data: å¸ç§æ•°æ®

    Returns:
        HAMA çŠ¶æ€å­—å…¸
    """
    change_pct = coin_data.get('change_percentage', 0)
    volume = coin_data.get('volume', 0)

    # ç®€å•è§„åˆ™
    if change_pct > 3 and volume > 1000000:
        status = 'strong_uptrend'
        trend = 'up'
        color = 'green'
        confidence = 'low'  # ä¼°ç®—çš„ç½®ä¿¡åº¦ä½
    elif change_pct > 1:
        status = 'uptrend'
        trend = 'up'
        color = 'green'
        confidence = 'low'
    elif change_pct < -3:
        status = 'strong_downtrend'
        trend = 'down'
        color = 'red'
        confidence = 'low'
    elif change_pct < -1:
        status = 'downtrend'
        trend = 'down'
        color = 'red'
        confidence = 'low'
    else:
        status = 'sideways'
        trend = 'neutral'
        color = 'gray'
        confidence = 'low'

    return {
        'status': status,
        'trend': trend,
        'color': color,
        'cross_signal': 'none',
        'confidence': confidence,
        'method': 'estimated',
        'timestamp': datetime.now().isoformat()
    }


def _redis_available():
    """æ£€æŸ¥Redisæ˜¯å¦å¯ç”¨"""
    redis_client = get_redis_client()
    if not redis_client:
        return False
    try:
        redis_client.ping()
        return True
    except Exception:
        return False


def _redis_get(key):
    """ä»Redisè·å–æ•°æ®"""
    if not _redis_available():
        return None
    try:
        redis_client = get_redis_client()
        data = redis_client.get(key)
        if data:
            return json.loads(data)
        return None
    except Exception as e:
        logger.warning(f"Redisè·å–å¤±è´¥: {e}")
        return None


def _redis_set(key, data, ttl=300):
    """è®¾ç½®æ•°æ®åˆ°Redis"""
    if not _redis_available():
        return False
    try:
        redis_client = get_redis_client()
        redis_client.setex(key, ttl, json.dumps(data))
        return True
    except Exception as e:
        logger.warning(f"Redisè®¾ç½®å¤±è´¥: {e}")
        return False


# å†…å­˜ç¼“å­˜ (å¤‡ç”¨,å½“Redisä¸å¯ç”¨æ—¶)
_top_gainers_mem_cache = {
    'data': None,
    'timestamp': None,
    'duration': timedelta(minutes=3)
}

_perpetuals_mem_cache = {
    'data': None,
    'timestamp': None,
    'duration': timedelta(minutes=5)
}

_watchlist_mem_cache = {
    'data': None,
    'timestamp': None,
    'duration': timedelta(minutes=5)
}


@tradingview_scanner_bp.route('/watchlist', methods=['GET'])
def get_watchlist():
    """
    è·å–é»˜è®¤å…³æ³¨åˆ—è¡¨ (å¸¦5åˆ†é’Ÿç¼“å­˜)

    æŸ¥è¯¢å‚æ•°:
    - limit: é™åˆ¶è¿”å›æ•°é‡ (é»˜è®¤20)
    - refresh: å¼ºåˆ¶åˆ·æ–°ç¼“å­˜ (é»˜è®¤false)

    è¿”å›:
    {
        "success": true,
        "count": 20,
        "data": [...],
        "cached": true
    }
    """
    global _watchlist_cache

    try:
        limit = request.args.get('limit', 20, type=int)
        limit = min(limit, 100)  # æœ€å¤š100ä¸ª

        force_refresh = request.args.get('refresh', 'false', type=str).lower() == 'true'
        current_time = datetime.now()

        # æ£€æŸ¥ç¼“å­˜
        if not force_refresh and _watchlist_cache['data'] is not None:
            cache_age = current_time - _watchlist_cache['timestamp']
            if cache_age < _watchlist_cache['duration']:
                logger.info(f"ä½¿ç”¨ç¼“å­˜çš„watchlistæ•°æ® (ç¼“å­˜æ—¶é—´: {cache_age.seconds}ç§’)")
                watchlist = _watchlist_cache['data'][:limit]

                return jsonify({
                    'success': True,
                    'count': len(watchlist),
                    'data': watchlist,
                    'cached': True,
                    'cache_age_seconds': int(cache_age.total_seconds()),
                    'source': 'TradingView Default Watchlist (Cached)'
                })

        # é‡æ–°è·å–æ•°æ®
        logger.info(f"è·å–é»˜è®¤å…³æ³¨åˆ—è¡¨, limit={limit}")

        # è·å–å®Œæ•´æ•°æ®å¹¶ç¼“å­˜
        full_watchlist = get_default_watchlist(limit=100)
        _watchlist_cache['data'] = full_watchlist
        _watchlist_cache['timestamp'] = current_time

        watchlist = full_watchlist[:limit]

        return jsonify({
            'success': True,
            'count': len(watchlist),
            'data': watchlist,
            'cached': False,
            'source': 'TradingView Default Watchlist'
        })

    except Exception as e:
        logger.error(f"è·å–é»˜è®¤å…³æ³¨åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@tradingview_scanner_bp.route('/perpetuals', methods=['GET'])
def get_perpetuals():
    """
    è·å–å¸å®‰æ°¸ç»­åˆçº¦åˆ—è¡¨ (ä½¿ç”¨Redisç¼“å­˜)

    æŸ¥è¯¢å‚æ•°:
    - limit: é™åˆ¶è¿”å›æ•°é‡ (é»˜è®¤50)
    - refresh: å¼ºåˆ¶åˆ·æ–°ç¼“å­˜ (é»˜è®¤false)

    è¿”å›:
    {
        "success": true,
        "count": 50,
        "data": [...],
        "cached": true
    }
    """
    try:
        limit = request.args.get('limit', 50, type=int)
        limit = min(limit, 200)  # æœ€å¤š200ä¸ª

        force_refresh = request.args.get('refresh', 'false', type=str).lower() == 'true'
        current_time = datetime.now()

        # è·å–å¸ç§çº§åˆ«ç¼“å­˜ç®¡ç†å™¨
        tv_cache = get_tv_cache_manager() or get_tv_cache_manager_impl()

        # ä¼˜å…ˆä½¿ç”¨å¸ç§çº§åˆ« Redis ç¼“å­˜
        if not force_refresh and tv_cache and tv_cache.is_available():
            try:
                # è·å–æ‰€æœ‰å·²ç¼“å­˜çš„å¸ç§
                cached_symbols = tv_cache.get_all_cached_symbols()

                if cached_symbols and len(cached_symbols) > 0:
                    # æ‰¹é‡è·å–å¸ç§æ•°æ®
                    cached_coins = tv_cache.get_coins(cached_symbols)

                    if cached_coins and len(cached_coins) > 0:
                        # è½¬æ¢ä¸ºåˆ—è¡¨
                        perpetuals = list(cached_coins.values())

                        # æŒ‰æˆäº¤é‡æ’åº
                        perpetuals.sort(key=lambda x: x.get('volume', 0), reverse=True)

                        # é™åˆ¶è¿”å›æ•°é‡
                        perpetuals = perpetuals[:limit]

                        logger.info(f"ä½¿ç”¨å¸ç§çº§åˆ« Redis ç¼“å­˜: {len(perpetuals)} ä¸ªå¸ç§")

                        return jsonify({
                            'success': True,
                            'count': len(perpetuals),
                            'data': perpetuals,
                            'cached': True,
                            'cache_age_seconds': 0,
                            'source': 'TradingView Perpetuals (Coin-level Redis Cache)'
                        })
            except Exception as e:
                logger.warning(f"è¯»å–å¸ç§çº§åˆ«ç¼“å­˜å¤±è´¥: {e}")

        # æ£€æŸ¥å†…å­˜ç¼“å­˜ (å¤‡ç”¨)
        if not force_refresh and _perpetuals_mem_cache['data'] is not None:
            cache_age = current_time - _perpetuals_mem_cache['timestamp']
            if cache_age < _perpetuals_mem_cache['duration']:
                logger.info(f"ä½¿ç”¨å†…å­˜ç¼“å­˜çš„æ°¸ç»­åˆçº¦æ•°æ® (ç¼“å­˜æ—¶é—´: {cache_age.seconds}ç§’)")
                perpetuals = _perpetuals_mem_cache['data'][:limit]

                return jsonify({
                    'success': True,
                    'count': len(perpetuals),
                    'data': perpetuals,
                    'cached': True,
                    'cache_age_seconds': int(cache_age.total_seconds()),
                    'source': 'TradingView Perpetuals (Memory Cache)'
                })

        # é‡æ–°è·å–æ•°æ®
        logger.info(f"è·å–æ°¸ç»­åˆçº¦åˆ—è¡¨, limit={limit}")

        # è·å–å®Œæ•´æ•°æ®å¹¶ç¼“å­˜(æœ€å¤š200ä¸ª)
        full_perpetuals = get_top_perpetuals(limit=200)

        # å­˜å…¥å¸ç§çº§åˆ« Redis ç¼“å­˜
        try:
            if tv_cache and tv_cache.is_available():
                cached_count = tv_cache.set_coins(full_perpetuals, ttl=300)
                logger.info(f"æ°¸ç»­åˆçº¦æ•°æ®å·²å­˜å…¥å¸ç§çº§åˆ« Redis ç¼“å­˜: {cached_count} ä¸ªå¸ç§")
            else:
                logger.warning("å¸ç§çº§åˆ«ç¼“å­˜ç®¡ç†å™¨ä¸å¯ç”¨")
        except Exception as e:
            logger.warning(f"å­˜å…¥å¸ç§çº§åˆ« Redis ç¼“å­˜å¤±è´¥: {e}")

        # å­˜å…¥å†…å­˜ç¼“å­˜
        _perpetuals_mem_cache['data'] = full_perpetuals
        _perpetuals_mem_cache['timestamp'] = current_time

        perpetuals = full_perpetuals[:limit]

        return jsonify({
            'success': True,
            'count': len(perpetuals),
            'data': perpetuals,
            'cached': False,
            'source': 'TradingView Perpetuals'
        })

    except Exception as e:
        logger.error(f"è·å–æ°¸ç»­åˆçº¦åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@tradingview_scanner_bp.route('/top-gainers', methods=['GET'])
def get_gainers():
    """
    è·å–æ¶¨å¹…æ¦œ (ä½¿ç”¨Redisç¼“å­˜)

    æŸ¥è¯¢å‚æ•°:
    - limit: é™åˆ¶è¿”å›æ•°é‡ (é»˜è®¤20)
    - min_change: æœ€å°æ¶¨è·Œå¹…ç™¾åˆ†æ¯” (å¯é€‰)
    - refresh: å¼ºåˆ¶åˆ·æ–°ç¼“å­˜ (é»˜è®¤false)

    è¿”å›:
    {
        "success": true,
        "count": 20,
        "data": [...],
        "cached": true
    }
    """
    try:
        limit = request.args.get('limit', 20, type=int)
        limit = min(limit, 100)  # æœ€å¤š100ä¸ª

        min_change = request.args.get('min_change', type=float)
        force_refresh = request.args.get('refresh', 'false', type=str).lower() == 'true'

        current_time = datetime.now()

        # è·å–å¸ç§çº§åˆ«ç¼“å­˜ç®¡ç†å™¨
        tv_cache = get_tv_cache_manager() or get_tv_cache_manager_impl()

        # ä¼˜å…ˆä½¿ç”¨å¸ç§çº§åˆ« Redis ç¼“å­˜
        if not force_refresh and tv_cache and tv_cache.is_available():
            try:
                # è·å–æ‰€æœ‰å·²ç¼“å­˜çš„å¸ç§
                cached_symbols = tv_cache.get_all_cached_symbols()

                if cached_symbols and len(cached_symbols) > 0:
                    # æ‰¹é‡è·å–å¸ç§æ•°æ®
                    cached_coins = tv_cache.get_coins(cached_symbols)

                    if cached_coins and len(cached_coins) > 0:
                        # è½¬æ¢ä¸ºåˆ—è¡¨
                        gainers = list(cached_coins.values())

                        # æŒ‰æ¶¨è·Œå¹…æ’åº
                        gainers.sort(key=lambda x: x.get('change_percentage', 0), reverse=True)

                        # åº”ç”¨è¿‡æ»¤
                        if min_change is not None:
                            gainers = [g for g in gainers if g.get('change_percentage', 0) >= min_change]

                        # é™åˆ¶è¿”å›æ•°é‡
                        gainers = gainers[:limit]

                        logger.info(f"ä½¿ç”¨å¸ç§çº§åˆ« Redis ç¼“å­˜: {len(gainers)} ä¸ªå¸ç§")

                        return jsonify({
                            'success': True,
                            'count': len(gainers),
                            'data': gainers,
                            'cached': True,
                            'cache_age_seconds': 0,
                            'source': 'TradingView Scanner - Top Gainers (Coin-level Redis Cache)'
                        })
            except Exception as e:
                logger.warning(f"è¯»å–å¸ç§çº§åˆ«ç¼“å­˜å¤±è´¥: {e}")

        # æ£€æŸ¥å†…å­˜ç¼“å­˜ (å¤‡ç”¨)
        if not force_refresh and _top_gainers_mem_cache['data'] is not None:
            cache_age = current_time - _top_gainers_mem_cache['timestamp']
            if cache_age < _top_gainers_mem_cache['duration']:
                logger.info(f"ä½¿ç”¨å†…å­˜ç¼“å­˜çš„æ¶¨å¹…æ¦œæ•°æ® (ç¼“å­˜æ—¶é—´: {cache_age.seconds}ç§’)")
                gainers = _top_gainers_mem_cache['data'].copy()

                # åº”ç”¨è¿‡æ»¤
                if min_change is not None:
                    gainers = [g for g in gainers if g.get('change_percentage', 0) >= min_change]

                gainers = gainers[:limit]

                logger.info(f"ä½¿ç”¨å†…å­˜ç¼“å­˜,å‡†å¤‡ä¸º {len(gainers)} ä¸ªå¸ç§æ·»åŠ  HAMA çŠ¶æ€...")

                # æ·»åŠ  HAMA çŠ¶æ€å­—æ®µ (ä½¿ç”¨ OCR è¯†åˆ«)
                for i, gainer in enumerate(gainers):
                    logger.info(f"æ­£åœ¨ä¸ºç¬¬ {i+1}/{len(gainers)} ä¸ªå¸ç§ {gainer.get('symbol')} æ·»åŠ  HAMA çŠ¶æ€...")
                    hama_status = _estimate_hama_status(gainer)
                    gainer['hama_status'] = hama_status
                    logger.info(f"âœ… {gainer.get('symbol')} HAMA çŠ¶æ€: {hama_status.get('status', 'N/A')}, æ–¹æ³•: {hama_status.get('method', 'N/A')}")

                return jsonify({
                    'success': True,
                    'count': len(gainers),
                    'data': gainers,
                    'cached': True,
                    'cache_age_seconds': int(cache_age.total_seconds()),
                    'source': 'TradingView Scanner - Top Gainers (Memory Cache)'
                })

        # ç¼“å­˜å¤±æ•ˆæˆ–å¼ºåˆ¶åˆ·æ–°,é‡æ–°è·å–æ•°æ®
        logger.info(f"è·å–æ¶¨å¹…æ¦œ, limit={limit}, min_change={min_change}")

        gainers = get_top_gainers(limit=100)  # è·å–æ›´å¤šç„¶åè¿‡æ»¤

        # å­˜å…¥å¸ç§çº§åˆ« Redis ç¼“å­˜
        try:
            if tv_cache and tv_cache.is_available():
                cached_count = tv_cache.set_coins(gainers, ttl=180)
                logger.info(f"æ¶¨å¹…æ¦œæ•°æ®å·²å­˜å…¥å¸ç§çº§åˆ« Redis ç¼“å­˜: {cached_count} ä¸ªå¸ç§")
            else:
                logger.warning("å¸ç§çº§åˆ«ç¼“å­˜ç®¡ç†å™¨ä¸å¯ç”¨")
        except Exception as e:
            logger.warning(f"å­˜å…¥å¸ç§çº§åˆ« Redis ç¼“å­˜å¤±è´¥: {e}")

        # å­˜å…¥å†…å­˜ç¼“å­˜
        _top_gainers_mem_cache['data'] = gainers
        _top_gainers_mem_cache['timestamp'] = current_time

        # åº”ç”¨è¿‡æ»¤
        if min_change is not None:
            gainers = [g for g in gainers if g.get('change_percentage', 0) >= min_change]

        gainers = gainers[:limit]

        logger.info(f"é‡æ–°è·å–æ•°æ®,å‡†å¤‡ä¸º {len(gainers)} ä¸ªå¸ç§æ·»åŠ  HAMA çŠ¶æ€...")

        # æ·»åŠ  HAMA çŠ¶æ€å­—æ®µ (ä½¿ç”¨ OCR è¯†åˆ«)
        for i, gainer in enumerate(gainers):
            logger.info(f"æ­£åœ¨ä¸ºç¬¬ {i+1}/{len(gainers)} ä¸ªå¸ç§ {gainer.get('symbol')} æ·»åŠ  HAMA çŠ¶æ€...")
            hama_status = _estimate_hama_status(gainer)
            gainer['hama_status'] = hama_status
            logger.info(f"âœ… {gainer.get('symbol')} HAMA çŠ¶æ€: {hama_status.get('status', 'N/A')}, æ–¹æ³•: {hama_status.get('method', 'N/A')}")

        return jsonify({
            'success': True,
            'count': len(gainers),
            'data': gainers,
            'cached': False,
            'source': 'TradingView Scanner - Top Gainers'
        })

    except Exception as e:
        logger.error(f"è·å–æ¶¨å¹…æ¦œå¤±è´¥: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@tradingview_scanner_bp.route('/symbols', methods=['POST'])
def get_symbols_data():
    """
    è·å–æŒ‡å®šå¸ç§çš„æ•°æ®

    è¯·æ±‚ä½“:
    {
        "symbols": ["BINANCE:BTCUSDT", "BINANCE:ETHUSDT", ...]
    }

    è¿”å›:
    {
        "success": true,
        "count": 2,
        "data": [...]
    }
    """
    try:
        data = request.get_json()

        if not data or 'symbols' not in data:
            return jsonify({
                'success': False,
                'error': 'è¯·æä¾›symbolsåˆ—è¡¨'
            }), 400

        symbols = data['symbols']

        if not isinstance(symbols, list):
            return jsonify({
                'success': False,
                'error': 'symbolså¿…é¡»æ˜¯æ•°ç»„'
            }), 400

        symbols = symbols[:100]  # æœ€å¤š100ä¸ª

        logger.info(f"è·å–æŒ‡å®šå¸ç§æ•°æ®, æ•°é‡={len(symbols)}")

        api = TradingViewScannerAPI()
        result = api.get_crypto_data(symbols)

        return jsonify({
            'success': True,
            'count': len(result),
            'data': result,
            'source': 'TradingView Scanner'
        })

    except Exception as e:
        logger.error(f"è·å–æŒ‡å®šå¸ç§æ•°æ®å¤±è´¥: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@tradingview_scanner_bp.route('/stats', methods=['GET'])
def get_stats():
    """
    è·å–ç»Ÿè®¡ä¿¡æ¯

    è¿”å›:
    {
        "success": true,
        "data": {
            "total_perpetuals": 200,
            "top_gainers": [...]
        }
    }
    """
    try:
        logger.info("è·å–ç»Ÿè®¡ä¿¡æ¯")

        # è·å–ä¸€äº›ç»Ÿè®¡æ•°æ®
        api = TradingViewScannerAPI()

        # è·å–å‰20ä¸ªä½œä¸ºæ ·æœ¬
        sample = api.get_default_watchlist(limit=20)

        # è®¡ç®—ç»Ÿè®¡
        if sample:
            avg_change = sum(c.get('change_percentage', 0) for c in sample) / len(sample)
            gainers_count = sum(1 for c in sample if c.get('change_percentage', 0) > 0)
            losers_count = sum(1 for c in sample if c.get('change_percentage', 0) < 0)

            stats_data = {
                'sample_size': len(sample),
                'avg_change': round(avg_change, 2),
                'gainers_count': gainers_count,
                'losers_count': losers_count,
                'top_gainer': max(sample, key=lambda x: x.get('change_percentage', 0)) if sample else None,
                'top_loser': min(sample, key=lambda x: x.get('change_percentage', 0)) if sample else None
            }
        else:
            stats_data = {}

        return jsonify({
            'success': True,
            'data': stats_data,
            'source': 'TradingView Scanner'
        })

    except Exception as e:
        logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@tradingview_scanner_bp.route('/chart-screenshot', methods=['GET'])
def get_chart_screenshot():
    """
    è·å– TradingView å›¾è¡¨æˆªå›¾

    Parameters:
        symbol: å¸ç§ç¬¦å· (å¦‚ BTCUSDT)
        interval: æ—¶é—´å‘¨æœŸ (é»˜è®¤ 15m)
        force_refresh: å¼ºåˆ¶åˆ·æ–° (é»˜è®¤ false)

    Returns:
        JSON with screenshot base64 data
    """
    try:
        symbol = request.args.get('symbol', 'BTCUSDT')
        interval = request.args.get('interval', '15m')
        force_refresh = request.args.get('force_refresh', 'false').lower() == 'true'

        logger.info(f"æ­£åœ¨è·å– {symbol} çš„å›¾è¡¨æˆªå›¾... (force_refresh={force_refresh})")

        # å¦‚æœä¸æ˜¯å¼ºåˆ¶åˆ·æ–°,å…ˆå°è¯•ä»ç¼“å­˜è·å–
        if not force_refresh:
            cached_image = _get_screenshot_from_cache(symbol, interval)
            if cached_image:
                logger.info(f"âœ… {symbol} ä»ç¼“å­˜è·å–æˆªå›¾æˆåŠŸ")
                return jsonify({
                    'success': True,
                    'symbol': symbol,
                    'interval': interval,
                    'image_base64': cached_image,
                    'content_type': 'image/png',
                    'cached': True
                })

        # å¼ºåˆ¶åˆ·æ–°æˆ–ç¼“å­˜æœªå‘½ä¸­,å®æ—¶æˆªå›¾
        if force_refresh:
            logger.info(f"ğŸ”„ {symbol} å¼ºåˆ¶åˆ·æ–°,æ­£åœ¨å®æ—¶æˆªå›¾...")
        else:
            logger.info(f"âš ï¸ {symbol} ç¼“å­˜æœªå‘½ä¸­,æ­£åœ¨å®æ—¶æˆªå›¾...")

        # ä½¿ç”¨æˆªå›¾å‡½æ•°
        success = _capture_and_cache_screenshot(symbol, interval)

        if success:
            # ä»ç¼“å­˜è¯»å–åˆšåˆšä¿å­˜çš„æˆªå›¾
            cached_image = _get_screenshot_from_cache(symbol, interval)
            if cached_image:
                logger.info(f"âœ… {symbol} å®æ—¶æˆªå›¾æˆåŠŸ")
                return jsonify({
                    'success': True,
                    'symbol': symbol,
                    'interval': interval,
                    'image_base64': cached_image,
                    'content_type': 'image/png',
                    'cached': False
                })
            else:
                logger.error(f"âŒ {symbol} æˆªå›¾æˆåŠŸä½†ç¼“å­˜è¯»å–å¤±è´¥")
                return jsonify({
                    'success': False,
                    'error': 'æˆªå›¾æˆåŠŸä½†ç¼“å­˜è¯»å–å¤±è´¥'
                }), 500
        else:
            logger.error(f"âŒ {symbol} æˆªå›¾å¤±è´¥")
            return jsonify({
                'success': False,
                'error': 'æˆªå›¾å¤±è´¥'
            }), 500

    except Exception as e:
        logger.error(f"è·å–å›¾è¡¨æˆªå›¾å¤±è´¥: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@tradingview_scanner_bp.route('/screenshot-cache/stats', methods=['GET'])
def get_screenshot_cache_stats():
    """
    è·å–æˆªå›¾ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯

    Returns:
        JSON with cache statistics
    """
    try:
        screenshot_cache = get_screenshot_cache()
        stats = screenshot_cache.get_stats()

        return jsonify({
            'success': True,
            'data': stats
        })
    except Exception as e:
        logger.error(f"è·å–æˆªå›¾ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@tradingview_scanner_bp.route('/screenshot-cache/cleanup', methods=['POST'])
def cleanup_screenshot_cache():
    """
    æ¸…ç†æ—§æˆªå›¾ç¼“å­˜

    Parameters:
        days: ä¿ç•™å¤©æ•° (é»˜è®¤ 7)

    Returns:
        JSON with cleanup results
    """
    try:
        from flask import request
        data = request.get_json() or {}
        days = data.get('days', 7)

        screenshot_cache = get_screenshot_cache()
        deleted_count = screenshot_cache.cleanup_old_screenshots(days)

        return jsonify({
            'success': True,
            'deleted_count': deleted_count,
            'message': f'å·²æ¸…ç† {deleted_count} æ¡è¶…è¿‡ {days} å¤©çš„æˆªå›¾'
        })
    except Exception as e:
        logger.error(f"æ¸…ç†æˆªå›¾ç¼“å­˜å¤±è´¥: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500
